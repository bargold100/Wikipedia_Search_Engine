# -*- coding: utf-8 -*-
"""IndexesCreation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TIqAid2O_sJYsPWdmbi5PMpU04hXt74L
"""

import math

#========IMPORTS ============
import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage

import hashlib
def _hash(s):
    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()

nltk.download('stopwords')

#from inverted_index_gcp import *

# These will already be installed in the testing environment so disregard the 
# amount of time (~1 minute) it takes to install. 
!pip install -q pyspark
!pip install -U -q PyDrive
!apt-get update -qq
!apt install openjdk-8-jdk-headless -qq
!pip install -q graphframes
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'
spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'
!wget -N -P $spark_jars $graphframes_jar

import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from graphframes import *
# Initializing spark context
# create a spark context and session
conf = SparkConf().set("spark.ui.port", "4050")
sc = pyspark.SparkContext(conf=conf)
sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))
spark = SparkSession.builder.getOrCreate()

spark
#================================= END IMPORTS =================================

from inverted_index_gcp import  *

##=====IMPORTS====
# if the following command generates an error, you probably didn't enable 
# the cluster security option "Allow API access to all Google Cloud services"
# under Manage Security â†’ Project Access when setting up the cluster
!gcloud dataproc clusters list --region us-central1

#=======LOADING BUCKET=====
# Put your bucket name below and make sure you can access it without an error
bucket_name = 'ir205973837' 
client = storage.Client()
blobs = client.list_blobs(bucket_name)

# Commented out IPython magic to ensure Python compatibility.
#=========LOADING THE INVERTED INDEX GCP PYTHON FILE======
# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir
# %cd -q /home/dataproc
!ls inverted_index_gcp.py



# adding our python module to the cluster
sc.addFile("/home/dataproc/inverted_index_gcp.py")
sys.path.insert(0,SparkFiles.getRootDirectory())


from inverted_index_gcp import InvertedIndex

full_path = "gs://wikidata_preprocessed/*"
parquetFile = spark.read.parquet(full_path)

# Count number of wiki pages
parquetFile.count()

#===============TOKENIZER OF GCP PART HOMEWORK ======

english_stopwords = frozenset(stopwords.words('english'))
corpus_stopwords = ["category", "references", "also", "external", "links", 
                    "may", "first", "see", "history", "people", "one", "two", 
                    "part", "thumb", "including", "second", "following", 
                    "many", "however", "would", "became"]

all_stopwords = english_stopwords.union(corpus_stopwords)
RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)

NUM_BUCKETS = 124
def token2bucket_id(token):
  return int(_hash(token),16) % NUM_BUCKETS


def tokenize(text):
  return [token.group() for token in RE_WORD.finditer(text.lower())]
  
def filter_tokens(tokens):
  ''' The function takes a list of tokens, filters out `all_stopwords`'''
  filtered_tokens =[]
  for word in tokens:
      if (word in all_stopwords):
        continue
      else:
        filtered_tokens.append(word)

  return filtered_tokens

#======================FUNCTIONS DEFINITIONS=============

def word_count(text, id):
  ''' Count the frequency of each word in `text` (tf) that is not included in 
  `all_stopwords` and return entries that will go into our posting lists. 
  Parameters:
  -----------
    text: str
      Text of one document
    id: int
      Document id
  Returns:
  --------
    List of tuples
      A list of (token, (doc_id, tf)) pairs 
      for example: [("Anarchism", (12, 5)), ...]
  '''
  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
  tokens = filter_tokens(tokens)
  # YOUR CODE HERE
  word_counts_counter = Counter(tokens)
  word_counts_list = word_counts_counter.items()
  list_to_return = []
  for item in word_counts_list:
    list_to_return.append((item[0], (id, item[1])))
  #raise NotImplementedError()
  return list_to_return

def FirstElement(elem):
  return elem[0]

def reduce_word_counts(unsorted_pl):
  ''' Returns a sorted posting list by wiki_id.
  Parameters:
  -----------
    unsorted_pl: list of tuples
      A list of (wiki_id, tf) tuples 
  Returns:
  --------
    list of tuples
      A sorted posting list.
  '''
  # YOUR CODE HERE
  sorted_pl = sorted(unsorted_pl, key = FirstElement)
  #raise NotImplementedError()
  return sorted_pl
  
  

def calculate_df(postings):
  ''' Takes a posting list RDD and calculate the df for each token.
  Parameters:
  -----------
    postings: RDD
      An RDD where each element is a (token, posting_list) pair.
  Returns:
  --------
    RDD
      An RDD where each element is a (token, df) pair.
  '''
  # YOUR CODE HERE
  #raise NotImplementedError()
  return postings.map(lambda x: (x[0], len(x[1])))

NUM_BUCKETS = 124
def token2bucket_id(token):
  return int(_hash(token),16) % NUM_BUCKETS

def partition_postings_and_write(postings, index_name):
  ''' A function that partitions the posting lists into buckets, writes out 
  all posting lists in a bucket to disk, and returns the posting locations for 
  each bucket. Partitioning should be done through the use of `token2bucket` 
  above. Writing to disk should use the function  `write_a_posting_list`, a 
  static method implemented in inverted_index_colab.py under the InvertedIndex 
  class. 
  Parameters:
  -----------
    postings: RDD
      An RDD where each item is a (w, posting_list) pair.
  Returns:
  --------
    RDD
      An RDD where each item is a posting locations dictionary for a bucket. The
      posting locations maintain a list for each word of file locations and 
      offsets its posting list was written to. See `write_a_posting_list` for 
      more details.
  '''
  # YOUR CODE HERE
  #raise NotImplementedError()
  my_rdd = postings.map(lambda x: (token2bucket_id(x[0]), (x)))
  result = my_rdd.groupByKey()
  return result.map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name, index_name))

def create_posting_locs_dict(prefix):
    # collect all posting lists locations into one super-set
    super_posting_locs = defaultdict(list)
    for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp/{prefix}'):
      if not blob.name.endswith("posting_locs.pickle"):
        continue
      with blob.open("rb") as f:
        posting_locs = pickle.load(f)
        for k, v in posting_locs.items():
          super_posting_locs[k].extend(v)
    return super_posting_locs

#=========HELPER FUNCTIONS=====

def sum_list(my_list):
  my_sum = 0
  for num in my_list:
    my_sum+=num
  return my_sum

def cala_idf(df):
  return math.log((text_n)/(df),10)

#==================== START IMPORTING DATA BY SPARK ===========
doc2title = parquetFile.select("title", "id").rdd
doc2title_dict = doc2title.collectAsMap()

#====================getting data from wikipedia TO 4 RDD'S ===========
doc_text_pairs = parquetFile.select("text", "id").rdd

doc_title_pairs = parquetFile.select("title", "id").rdd

pages_links = parquetFile.select("id", "anchor_text").rdd
doc_anchor_pairs = parquetFile.select("anchor_text", "id").rdd.map(lambda row: (row[1],[t[1] for t in row[0]],[t[0] for t in row[0]]))
doc_anchor_pairs = doc_anchor_pairs.map(lambda row: [(row[1][i], row[2][i]) for i in range(0,len(row[1]))])
doc_anchor_pairs = doc_anchor_pairs.flatMap(lambda x: x)

#===========TOKINIZING to convert text fields to lists anf FILTERING TO QUERY WORDS 

#rdd with tuple key=doc id, val= list of filtered tokens
doc_id_tokens_pairs_text = doc_text_pairs.map(lambda row: (row[1], filter_tokens(tokenize(row[0]), english_stopwords)))
#key =  doc id , val = len doc
doc_len_text = doc_id_tokens_pairs_text.map(lambda row: (row[0], len(row[1])))

doc_id_tokens_pairs_title = doc_title_pairs.map(lambda row: (row[1], filter_tokens(tokenize(row[0]), english_stopwords)))
#key =  doc id , val = len doc
doc_len_title = doc_id_tokens_pairs_title.map(lambda row: (row[0], len(row[1])))

doc_id_tokens_pairs_anchor = doc_anchor_pairs.map(lambda row: (row[1], filter_tokens(tokenize(row[0]), english_stopwords)))
#key =  doc id , val = len doc
doc_len_anchor = doc_id_tokens_pairs_anchor.map(lambda row: (row[0], len(row[1])))

#====================join the tokens back to strings ===============
doc_text_pairs = doc_id_tokens_pairs_text.map(lambda row: (" ".join(row[1]), row[0]))
doc_title_pairs = doc_id_tokens_pairs_title.map(lambda row: (" ".join(row[1]), row[0]))
doc_anchor_pairs = doc_id_tokens_pairs_anchor.map(lambda row: (" ".join(row[1]), row[0]))

#===================create the fields of the BODY inverted index===============

word_counts_text = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))
#posting list
postings_text = word_counts_text.groupByKey().mapValues(reduce_word_counts)
#filter rare words
postings_filtered_text = postings_text.filter(lambda x: len(x[1])>50)
#writing to disk
posting_locs_list_text = partition_postings_and_write(postings_filtered_text, 'text').collect()
# merge the posting locations into a single dict
posting_locs_dict_text = create_posting_locs_dict('text')
#df field in the inverted index
w2df_text = calculate_df(postings_filtered_text)
w2df_dict_text = w2df_text.collectAsMap()
#doc len text dict
doc_len_text_dict = doc_len_text.collectAsMap()
#calc N field
text_n = len(doc_len_text_dict)

#create norma field for the BODY  inverted index
norma_field_text = doc_text_pairs.map(lambda x: word_count(x[0], x[1]))
norma_field_text = norma_field_text.map(lambda row: (row, sum_list([elem[1][1] for elem in row])))
norma_field_text = norma_field_text.map(lambda row: [(item[0],item[1][0],item[1][1],row[1]) for item in row[0]])
norma_field_text = norma_field_text.map(lambda row: [(item[0],(item[1], item[2], item[3])) for item in row])
norma_field_text = norma_field_text.flatMap(lambda x: x)
norma_field_text = norma_field_text.groupByKey().mapValues(list)
norma_field_text = norma_field_text.map(lambda row: (row[0],len(row[1]),row[1]))
norma_field_text = norma_field_text.map(lambda row: ((cala_idf(row[1]))**2, [(elem[0], (elem[1]**2)/(elem[2]**2)) for elem in row[2]]))
norma_field_text = norma_field_text.map(lambda row: [(elem[0], elem[1]*row[0]) for elem in row[1]])
norma_field_text = norma_field_text.flatMap(lambda x: x)
norma_field_text = norma_field_text.reduceByKey(lambda a, b: a + b)
norma_field_text_dict = norma_field_text.map(lambda row: (row[0], math.sqrt(row[1]))).collectAsMap()

#calc idf field for the BODY
idf_text = w2df_text.map(lambda row: (row[0], cala_idf(row[1])))
idf_text_dict = idf_text.collectAsMap()

#================= create the fields of the TITLE inverted index ==========
word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))
#posting list
postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)
#filter rare words
postings_filtered_title = postings_title.filter(lambda x: len(x[1])>10)
#writing to disk
posting_locs_list_title = partition_postings_and_write(postings_filtered_title, 'title').collect()
# merge the posting locations into a single dict
posting_locs_dict_title = create_posting_locs_dict('title')
#df field in the inverted index
w2df_title = calculate_df(postings_filtered_title)
w2df_dict_title = w2df_title.collectAsMap()
#doc len title dict
doc_len_title_dict = doc_len_title.collectAsMap()
#calc idf field
title_n = len(doc_len_title_dict)

#calc idf field for TITLE
idf_title = w2df_title.map(lambda row: (row[0], cala_idf(row[1])))
idf_title_dict = idf_title.collectAsMap()

# ============== create the fields of the inverted ANCHOR index ==========
word_counts_anchor = doc_anchor_pairs.flatMap(lambda x: word_count(x[0], x[1]))
#posting list
postings_anchor = word_counts_anchor.groupByKey().mapValues(reduce_word_counts)
#filter rare words
postings_filtered_anchor = postings_anchor.filter(lambda x: len(x[1])>40)
#writing to disk
posting_locs_list_anchor = partition_postings_and_write(postings_filtered_anchor, 'anchor').collect()
# merge the posting locations into a single dict
posting_locs_dict_anchor = create_posting_locs_dict('anchor')
#df field in the inverted index
w2df_anchor = calculate_df(postings_filtered_anchor)
w2df_dict_anchor = w2df_anchor.collectAsMap()
#doc len anchor dict
#doc_len_anchor_dict = doc_len_anchor.collectAsMap()
#calc idf field
#anchor_n = len(doc_len_anchor_dict)

#calc idf field for anchor
idf_anchor = w2df_anchor.map(lambda row: (row[0], cala_idf(row[1])))
idf_anchor_dict = idf_anchor.collectAsMap()

#=============Create inverted index instance for BODY and writing to the storage======

inverted_text = InvertedIndex()
# Adding the posting locations dictionary to the inverted index
inverted_text.posting_locs = posting_locs_dict_text
# Add the token - df dictionary to the inverted index
inverted_text.df = w2df_dict_text
#add doc len field
inverted_text.doc_length = doc_len_text_dict
#add doc norma field
inverted_text.doc_norma = norma_field_text_dict
#inverted_text.doc_norma = norma_field_text_dict
#N field in the inverted index
inverted_text.N = text_n
#idf field in the inverted index
inverted_text.idf = idf_text_dict
# write the global stats out
inverted_text.write_index('.', 'index_text')
# upload to gs
index_src = "index_text.pkl"
index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'
!gsutil cp $index_src $index_dst

# =============Create inverted index instance for TITLE and writing to the storage  ============

inverted_title = InvertedIndex()
# Adding the posting locations dictionary to the inverted index
inverted_title.posting_locs = posting_locs_dict_title
# Add the token - df dictionary to the inverted index
inverted_title.df = w2df_dict_title
# write the global stats out
inverted_title.write_index('.', 'index_title')
# upload to gs
index_src = "index_title.pkl"
index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'
!gsutil cp $index_src $index_dst

# ==============Create inverted index instance for ANCHOR and writing to the storage  =============

inverted_anchor = InvertedIndex()
# Adding the posting locations dictionary to the inverted index
inverted_anchor.posting_locs = posting_locs_dict_anchor
# Add the token - df dictionary to the inverted index
inverted_anchor.df = w2df_dict_anchor
# write the global stats out
inverted_anchor.write_index('.', 'index_anchor')
# upload to gs
index_src = "index_anchor.pkl"
index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'
!gsutil cp $index_src $index_dst

from google.colab import auth
auth.authenticate_user()
from google.colab import drive
drive.mount('/content/drive')

#=========== HELPER FUNCTIONS TO READ PKL FILES ======

def write_disk(base_dir, name, my_object):
  with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:
      pickle.dump(my_object,f)

        
def read_from_disk(base_dir, name):
  with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:
    return pickle.load(f)

def generate_graph(pages):
  ''' Compute the directed graph generated by wiki links.
  Parameters:
  -----------
    pages: RDD
      An RDD where each row consists of one wikipedia articles with 'id' and 
      'anchor_text'.
  Returns:
  --------
    edges: RDD
      An RDD where each row represents an edge in the directed graph created by
      the wikipedia links. The first entry should the source page id and the 
      second entry is the destination page id. No duplicates should be present. 
    vertices: RDD
      An RDD where each row represents a vetrix (node) in the directed graph 
      created by the wikipedia links. No duplicates should be present. 
  '''
  # YOUR CODE HERE
  edges = pages.flatMap(lambda x: [(x[0] ,row.id) for row in x[1]]).distinct()
  vertices = edges.flatMap(lambda x : x).distinct()
  #raise NotImplementedError()
  vertices = vertices.map(lambda x : (x, ))
  return edges, vertices

#======  CALC PAGE RANK  =======
edges, vertices = generate_graph(pages_links)
edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')
verticesDF = vertices.toDF(['id']).repartition(4, 'id')
g = GraphFrame(verticesDF, edgesDF)
pr_results = g.pageRank(resetProbability=0.15, maxIter=10)
pr = pr_results.vertices.select("id", "pagerank").rdd.collectAsMap()
write_disk('.',"page_rank",pr)

#============= CALC PAGE VIEWS ======
# Paths
# Using user page views (as opposed to spiders and automated traffic) for the 
# month of August 2021
pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'
p = Path(pv_path) 
pv_name = p.name
pv_temp = f'{p.stem}-4dedup.txt'
pv_clean = f'{p.stem}.pkl'
# Download the file (2.3GB) 
!wget -N $pv_path
# Filter for English pages, and keep just two fields: article ID (3) and monthly 
# total number of page views (5). Then, remove lines with article id or page 
# view values that are not a sequence of digits.
!bzcat $pv_name | grep "^en\.wikipedia" | cut -d' ' -f3,5 | grep -P "^\d+\s\d+$" > $pv_temp
# Create a Counter (dictionary) that sums up the pages views for the same 
# article, resulting in a mapping from article id to total page views.
wid2pv = Counter()
with open(pv_temp, 'rt') as f:
  for line in f:
    parts = line.split(' ')
    wid2pv.update({int(parts[0]): int(parts[1])})
# write out the counter as binary file (pickle it)
with open(pv_clean, 'wb') as f:
  pickle.dump(wid2pv, f)